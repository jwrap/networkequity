{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# For Thesis\n",
    "### UNBAKED, YET TO BE INTEGRATED WITH THE CODES IN GITHUB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import ipyparallel\n",
    "\n",
    "# client=ipyparallel.Client()\n",
    "# client.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%px --local\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.pylab import *\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "import copy\n",
    "from rasterstats import zonal_stats\n",
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "sys.path.append('C:/Users/Lenovo/Desktop/bangladesh_network')\n",
    "\n",
    "import network_prep as net_p\n",
    "import network_visualization as net_v\n",
    "import od_prep as od_p\n",
    "import weighted_betweenness as betw_w\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from matplotlib.pylab import *\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "from __future__ import division\n",
    "\n",
    "import ema_workbench\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Network creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.py:2834: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# %%px --local\n",
    "# userpath = \"D:/Bramka's File/Bangladesh Project/\"\n",
    "# networkfile = 'Data preparation/BGD_road_network/Road_BGD_Class1_2_f_connected_v4.shp'\n",
    "# centroidfile = 'Data preparation/BGD_Districtdata_Citycentroid/BGD_Districtdata_Citycentroid_level2_v02.shp'\n",
    "# centroid = userpath+centroidfile\n",
    "# network = userpath+networkfile\n",
    "import os\n",
    "filepath = os.getcwd()\n",
    "network = filepath+'\\\\Road_BGD_Class1_2_f_connected_v4.shp'\n",
    "network = filepath+'\\\\rmms_v11_7_waterway_noZ2.shp'\n",
    "centroid = filepath+'\\\\BGD_Districtdata_Citycentroid_level2_v02.shp'\n",
    "\n",
    "\n",
    "gdf_points, gdf_node_pos, gdf = net_p.prepare_centroids_network(centroid, network)\n",
    "\n",
    "gdf['penalty'] = gdf.apply(lambda row: 15 if (row['cross']==1 and row['mode']=='road') else 0, axis=1)\n",
    "gdf['length'] = gdf['length'] + gdf['penalty']\n",
    "\n",
    "#simplify the graph\n",
    "G2_new = net_p.gdf_to_simplified_multidigraph(gdf_node_pos, gdf, undirected=True)\n",
    "\n",
    "#change to simple Graph object type\n",
    "G2_new_tograph = net_p.multigraph_to_graph(G2_new)\n",
    "\n",
    "#take the largest components\n",
    "for g in nx.connected_component_subgraphs(G2_new_tograph):\n",
    "    if len(list(g.edges())) > 100:\n",
    "        G3 = g\n",
    "        \n",
    "G2_new_tograph = G3.copy()\n",
    "#change the simplified transport network back to GeoDataFrame\n",
    "gdf2 = net_p.graph_to_df(G2_new_tograph)\n",
    "\n",
    "filepath = os.getcwd()\n",
    "adm_csv = filepath+'\\\\District_level_data_v7.csv'\n",
    "#adm_csv = filepath+'\\\\District_level_data_v5.csv'\n",
    "adm_shp = filepath+'\\\\BGD_adm2.shp'\n",
    "district_gdf2 = net_p.prepare_adm_background(adm_csv, adm_shp, ['Code', 'Population', 'Population_M', 'Total_export',\n",
    "                                                               'Jute_mill', 'Flour_mill', 'Tot_Garment_Factory', 'Household',\n",
    "                                                               'Land_throughput', 'SteelBricks_exp_ton', 'Food_exp_ton',\n",
    "                                                               'Jutextile_exp_ton', 'Garment_exp_ton', 'Textile_loc_ton',\n",
    "                                                               'Wheat_loc_ton', 'RawJute_loc_ton', 'Foods_loc_ton',\n",
    "                                                               'Nonfoods_loc_ton'])\n",
    "\n",
    "#embed district data to gdf_points\n",
    "\n",
    "#read district data\n",
    "district_data = pd.read_csv(adm_csv)\n",
    "\n",
    "#rename to HASC_2\n",
    "district_data.rename(columns={'Code':'HASC_2'}, inplace=True)\n",
    "\n",
    "#merge them\n",
    "gdf_points = pd.merge(gdf_points,district_data,on='HASC_2',how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-EMA preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%px --local\n",
    "#Alternative deterrence function\n",
    "\n",
    "def det_func_basic(distance):\n",
    "    #distance is a n x n DataFrame of euclidean distance between all centroids\n",
    "    distance = distance*distance\n",
    "    distance = 100000/distance\n",
    "    for i in list(distance.columns):\n",
    "        for j in list(distance.index.values):\n",
    "            if distance[i][j] > 9999999:\n",
    "                distance[i][j] = 0\n",
    "    return distance\n",
    "\n",
    "def det_func_prob(distance, beta=0.05):\n",
    "    #distance is a n x n DataFrame of euclidean distance between all centroids\n",
    "    haha = pd.DataFrame()\n",
    "    for i, row in distance.iterrows():\n",
    "        row2 = row * (-beta)\n",
    "        exp_row = np.exp(row2)\n",
    "        sum_exp = exp_row.sum()\n",
    "        exp_row = 100000 * (exp_row / sum_exp)\n",
    "        haha = haha.append(exp_row, ignore_index = True)\n",
    "        \n",
    "    haha.values[[np.arange(len(haha))]*2] = 0\n",
    "    return haha\n",
    "\n",
    "def det_func_sp(distance, G=G2_new_tograph, gdf_points=gdf_points, weight='length'):\n",
    "    centroid_list = list(gdf_points['Node'])\n",
    "    c = []\n",
    "    for i in arange(len(centroid_list)):\n",
    "        row_list = []\n",
    "        for j in arange(len(centroid_list)):\n",
    "            if i != j:\n",
    "                dist = nx.dijkstra_path_length(G=G, source=centroid_list[i], target=centroid_list[j], weight=weight)\n",
    "                row_list.append(dist)\n",
    "            else:\n",
    "                row_list.append(0)\n",
    "        c.append(row_list)\n",
    "    \n",
    "    \n",
    "    arr = np.arange(len(centroid_list))\n",
    "    det_df = pd.DataFrame(c, index=arr, columns=arr)\n",
    "                 \n",
    "    return det_df\n",
    "\n",
    "det_func = {1 : det_func_basic, 2 : det_func_prob, 3 : det_func_sp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%px --local\n",
    "theta = 50 #for m1\n",
    "beta = 0.5 #for m1, m6\n",
    "weight='length'\n",
    "cutoff = 0.05 #for m5_01\n",
    "m10_buffer = 0.005 #for m10\n",
    "penalty = 1.2 #for m8_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%px --local\n",
    "centroid_nodes = od_p.prepare_centroids_list(G2_new_tograph)\n",
    "\n",
    "#export OD\n",
    "prod_lists1 = ['SteelBricks_exp_ton', 'Food_exp_ton','Jutextile_exp_ton', 'Garment_exp_ton']\n",
    "attr_driver = 'Total_export'\n",
    "OD_export_dict = od_p.all_ods_creation(gdf_points = gdf_points, prod_lists = prod_lists1, \n",
    "                                       attr_driver = attr_driver, dist_deterrence = det_func[1])\n",
    "\n",
    "prod_lists2 = [ 'Foods_loc_ton', 'Nonfoods_loc_ton']\n",
    "attr_driver='Population_x'\n",
    "OD_local_dict1 = od_p.all_ods_creation(gdf_points = gdf_points, prod_lists = prod_lists2, \n",
    "                                       attr_driver = attr_driver, dist_deterrence = det_func[1])\n",
    "\n",
    "prod_lists3 = ['RawJute_loc_ton']\n",
    "attr_driver='Jute_mill'\n",
    "OD_local_dict2 = od_p.all_ods_creation(gdf_points = gdf_points, prod_lists = prod_lists3, \n",
    "                                       attr_driver = attr_driver, dist_deterrence = det_func[1])\n",
    "\n",
    "prod_lists4 = ['Wheat_loc_ton']\n",
    "attr_driver='Flour_mill'\n",
    "OD_local_dict3 = od_p.all_ods_creation(gdf_points = gdf_points, prod_lists = prod_lists4, \n",
    "                                       attr_driver = attr_driver, dist_deterrence = det_func[1])\n",
    "\n",
    "prod_lists5 = ['Textile_loc_ton']\n",
    "attr_driver='Tot_Garment_Factory'\n",
    "OD_local_dict4 = od_p.all_ods_creation(gdf_points = gdf_points, prod_lists = prod_lists5, \n",
    "                                       attr_driver = attr_driver, dist_deterrence = det_func[1])\n",
    "\n",
    "\n",
    "#Combine all ODs\n",
    "OD_local1 = OD_local_dict1[OD_local_dict1.keys()[0]]\n",
    "for i in range(len(OD_local_dict1)-1):\n",
    "    OD_local1 = OD_local1 +  OD_local_dict1[OD_local_dict1.keys()[i+1]]\n",
    "    \n",
    "OD_local2 = OD_local_dict2[OD_local_dict2.keys()[0]]\n",
    "\n",
    "OD_local3 = OD_local_dict3[OD_local_dict3.keys()[0]]\n",
    "\n",
    "OD_local4 = OD_local_dict4[OD_local_dict4.keys()[0]]\n",
    "\n",
    "OD_export = OD_export_dict[OD_export_dict.keys()[0]]\n",
    "for i in range(len(OD_export_dict)-1):\n",
    "    OD_export = OD_export +  OD_export_dict[OD_export_dict.keys()[i+1]]\n",
    "\n",
    "OD_all = OD_local1 + OD_local2 + OD_local3 + OD_local4 + OD_export\n",
    "\n",
    "#create unweighted OD\n",
    "centroid_district_listed = list(OD_all.columns)\n",
    "\n",
    "OD_unweighted = pd.DataFrame(1, index=centroid_district_listed, columns=centroid_district_listed)\n",
    "\n",
    "for i,row in OD_unweighted.iterrows():\n",
    "    OD_unweighted.loc[i][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%px --local\n",
    "#for m3_02\n",
    "division_gdf = gp.read_file('BGD_adm1.shp')\n",
    "division_gdf.crs = {'proj': 'longlat', 'ellps': 'WGS84', 'datum': 'WGS84'}\n",
    "division_gdf['HASC_1'] = division_gdf['HASC_1'].apply(lambda code: code.replace('.', '_'))\n",
    "division_list = list(set(list(division_gdf['HASC_1'])))\n",
    "division_gdf['geometry'] = division_gdf.geometry.apply(lambda x: x.buffer(0.01))\n",
    "def determine_division(geom, division):\n",
    "    for i, row in division.iterrows():\n",
    "        if geom.intersects(row['geometry']):\n",
    "            return row['HASC_1']\n",
    "gdf2['division'] = gdf2.geometry.apply(lambda geom: determine_division(geom=geom, division=division_gdf))\n",
    "div_gdf_list = []\n",
    "for div in division_list:\n",
    "    exec(\"{}_gdf = gdf2.loc[gdf2['geometry'].intersects(division_gdf.loc[division_gdf['HASC_1']==div]['geometry'].iloc[0])]\".format(div))\n",
    "    exec(\"div_gdf_list.append({}_gdf)\".format(div))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%px --local\n",
    "import pickle\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "ksp = load_obj('ksp_for_m402')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%px --local\n",
    "def _daily_accessibility(centroid, G, theta, weight='length', beta=0.5):\n",
    "    '''\n",
    "    Helper function for function interdiction_m1\n",
    "\n",
    "    return:\n",
    "    a              : daily accessibility index\n",
    "    len(sp_length) : number of nodes accessible within daily travel threshold (theta)\n",
    "    '''\n",
    "\n",
    "    total_sp_length = 0\n",
    "\n",
    "    #calculate shortest path length to all other centroids\n",
    "#     for target in G.nodes(): #why need this?\n",
    "    sp_length = nx.single_source_dijkstra_path_length(G=G, source=centroid, cutoff=theta, weight=weight)\n",
    "    count_node = 0\n",
    "#     for key, val in sp_length.iteritems():\n",
    "#         try:\n",
    "#             total_sp_length += 1 / (val**beta)\n",
    "#         except:\n",
    "#             pass\n",
    "#         count_node += 1\n",
    "    for item in sp_length:\n",
    "        try:\n",
    "            total_sp_length += 1 / (item[1]**beta)\n",
    "        except:\n",
    "            pass\n",
    "        count_node += 1\n",
    "\n",
    "    #calculate the accessibility\n",
    "    try:\n",
    "        a = total_sp_length\n",
    "    except:\n",
    "        a = 0\n",
    "\n",
    "    return a, count_node\n",
    "\n",
    "def _sum_daily_accessibility(a_dict, a_n_dict):\n",
    "    '''\n",
    "    Helper function for function interdiction_m1\n",
    "    '''\n",
    "    sum_a = sum(a_dict.values())\n",
    "\n",
    "    sum_a_n = sum(a_n_dict.values())\n",
    "\n",
    "    return sum_a, sum_a_n\n",
    "\n",
    "def _dict_daily_accessibility(centroids, G, theta, weight='length', beta=0.5):\n",
    "    '''\n",
    "    Helper function for function interdiction_m1\n",
    "\n",
    "    return:\n",
    "    a_dict     : dictionary of daily accessibility, keyed by centroids id\n",
    "    a_n_dict   : dictionary of number of nodes accessible within daily travel threshold, keyed by centroids id\n",
    "    '''\n",
    "\n",
    "    a_dict = {}\n",
    "    a_n_dict = {}\n",
    "    for centroid in centroids:\n",
    "        a, a_n = _daily_accessibility(centroid=centroid, G=G, theta=theta, weight=weight, beta=beta)\n",
    "        a_dict.update({centroid:a})\n",
    "        a_n_dict.update({centroid:a_n})\n",
    "        \n",
    "    return a_dict, a_n_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%px --local\n",
    "#EMA building block\n",
    "sp_dict_graph = betw_w.sp_dict_graph_creation(G=G2_new_tograph, sources=centroid_nodes, \n",
    "                                              targets=centroid_nodes, weight='length')\n",
    "a_dict_base, a_n_dict_base = _dict_daily_accessibility(centroids=centroid_nodes, G=G2_new_tograph, theta=theta, \n",
    "                                                                 weight='length', beta=beta)\n",
    "sum_a_base, sum_a_n_base = _sum_daily_accessibility(a_dict_base, a_n_dict_base)\n",
    "all_daily_sp_list, all_daily_sp_dict = betw_w._all_daily_sp_record(G=G2_new_tograph, sources=centroid_nodes, \n",
    "                                                            cutoff=theta, weight='length')\n",
    "\n",
    "#EMA building block\n",
    "div_graph_list = []\n",
    "for div in division_list:\n",
    "    exec(\"{}_graph = nx.from_pandas_dataframe(df={}_gdf, source='FNODE_', target='TNODE_',edge_attr='length')\".format(div,div))\n",
    "    exec(\"div_graph_list.append({}_graph)\".format(div))\n",
    "div_graph_dict = dict(zip(division_list, div_graph_list))\n",
    "div_init_avrgdist_dict = {}\n",
    "for key, val in div_graph_dict.iteritems():\n",
    "    cc = 0\n",
    "    if nx.number_connected_components(val) > 1:\n",
    "        for subgraph in nx.connected_component_subgraphs(val):\n",
    "            if len(subgraph) > cc:\n",
    "                cc = len(subgraph)\n",
    "                graph = subgraph\n",
    "    else:\n",
    "        graph = val\n",
    "    average_sp_length = nx.average_shortest_path_length(graph, weight='length')\n",
    "    div_init_avrgdist_dict.update({key:average_sp_length})\n",
    "    \n",
    "#EMA building block\n",
    "total_cost_base, od_cost_dict = betw_w._total_cost_sp(G=G2_new_tograph, sources=centroid_nodes, targets=centroid_nodes,\n",
    "                                              weight=weight, od=OD_all, weighted=False)\n",
    "total_cost_base_w, od_cost_dict_w = betw_w._total_cost_sp(G=G2_new_tograph, sources=centroid_nodes, targets=centroid_nodes,\n",
    "                                              weight=weight, od=OD_all, weighted=True)\n",
    "total_cost_sp_inversed, od_cost_inversed_dict =  betw_w._total_cost_sp_inversed(G=G2_new_tograph, sources=centroid_nodes, \n",
    "                                                                         targets=centroid_nodes, weight=weight)\n",
    "efficiency_base = betw_w._network_efficiency_calc(G=G2_new_tograph, total_cost_inversed=total_cost_sp_inversed)\n",
    "\n",
    "#EMA building block\n",
    "flow_fromto_df = pd.DataFrame(OD_all.sum(axis=0)+OD_all.sum(axis=1))\n",
    "flow_fromto_df.columns = ['flow']\n",
    "a_sum_base, a_master_dict =  betw_w._sum_weighted_accessibility(G=G2_new_tograph, centroids=centroid_nodes,\n",
    "                                                                flow=flow_fromto_df, weight='length', beta=beta)\n",
    "\n",
    "#EMA building block\n",
    "sp_cost_dict = betw_w._shortest_path_cost(G=G2_new_tograph, centroids=centroid_nodes, weight='length')\n",
    "\n",
    "#EMA building block\n",
    "flood_file = 'fluvial_defended_1in75_tile_1_-9999to0.tif'\n",
    "gdf3 = gdf2.copy()\n",
    "gdf3['geometry'] = gdf3.geometry.apply(lambda geom: geom.buffer(m10_buffer))\n",
    "zonal_stats_dict = zonal_stats(gdf3, flood_file, stats='mean', all_touched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#EMA building block\n",
    "flood_file = 'fluvial_defended_1in75_tile_1_-9999to0.tif'\n",
    "gdf3 = gdf2.copy()\n",
    "gdf3['geometry'] = gdf3.geometry.apply(lambda geom: geom.buffer(m10_buffer))\n",
    "zonal_stats_dict = zonal_stats(gdf3, flood_file, stats='mean', all_touched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%px --local\n",
    "def no_interdiction(G, centroids, od, od_unw, weight, gdf=gdf2, div_graph_dict=div_graph_dict,\n",
    "                    div_init_avrgdist_dict=div_init_avrgdist_dict, cutoff=cutoff, penalty=penalty,\n",
    "                    zonal_stats_dict=zonal_stats_dict):\n",
    "    \n",
    "    edgelist = []\n",
    "    for edge in list(G.edges()):\n",
    "        edgelist.append(edge)\n",
    "    \n",
    "    #for m3_01\n",
    "    flow_unweighted = betw_w.aon_assignment(G=G, sources=centroids,\n",
    "                                            targets=centroids, weight=weight, od=od_unw)\n",
    "    m3_01_dict = betw_w.edge_betweenness_centrality(flow_unweighted, od_unw)\n",
    "    \n",
    "    #for m3_02\n",
    "    gdf['m3_02'] = gdf.apply(lambda row: betw_w.interdiction_m3_02(row=row, div_graph_dict=div_graph_dict,\n",
    "                                                                   div_init_avrgdist_dict=div_init_avrgdist_dict), axis=1)\n",
    "    gdf['FromToTuple'] = gdf.apply(lambda row: tuple([row['FNODE_'], row['TNODE_']]), axis=1)\n",
    "    for i in list(gdf['FromToTuple']):\n",
    "        if not i in m3_01_dict.keys():\n",
    "            idx = gdf.loc[gdf['FromToTuple']==i].index[0]\n",
    "            new_tup = tuple([i[1],i[0]])\n",
    "            gdf['FromToTuple'][idx] = new_tup\n",
    "    el = list(gdf['FromToTuple'])\n",
    "    m3_02 = list(gdf['m3_02'])\n",
    "    m3_02_dict = dict(zip(el, m3_02))\n",
    "    \n",
    "    #for m5_01\n",
    "    gdf['m5_01'] = gdf.geometry.apply(lambda line: betw_w.m5_01(gdf=gdf, line=line, cutoff=cutoff))\n",
    "    m5_01 = list(gdf['m5_01'])\n",
    "    m5_01_dict = dict(zip(el, m5_01))\n",
    "    \n",
    "    #for m8_02\n",
    "    flow_probit_5 = betw_w.probit_assignment(G=G, sources=centroids, targets=centroids, weight=weight, \n",
    "                                             od=od, N=4, penalty=penalty)\n",
    "    m8_02_dict = betw_w.edge_betweenness_centrality(flow_probit_5, od)\n",
    "    \n",
    "    #for m10\n",
    "    gdf['m10'] = [d.values()[0] for d in zonal_stats_dict]\n",
    "    gdf['m10'] = gdf.m10.apply(lambda val: 0 if val > 0.8 else val)\n",
    "    m10 = list(gdf['m10'])\n",
    "    m10_dict = dict(zip(el, m10))    \n",
    "    \n",
    "    #if an edge does not have value yet\n",
    "    #assign 0 to it\n",
    "    all_dicts = [m3_01_dict, m3_02_dict, m5_01_dict, m8_02_dict, m10_dict]\n",
    "    for edge in edgelist:\n",
    "        for metric in all_dicts:\n",
    "            if not edge in metric:\n",
    "                metric.update({edge:0})\n",
    "    \n",
    "    return m3_01_dict, m3_02_dict, m5_01_dict, m8_02_dict, m10_dict\n",
    "\n",
    "def interdiction(G, centroids, od, weight, theta=theta, beta=beta,\n",
    "                 a_dict_base=a_dict_base, a_n_dict_base=a_n_dict_base, sum_a_base=sum_a_base, sum_a_n_base=sum_a_n_base,\n",
    "                 all_daily_sp_list=all_daily_sp_list, all_daily_sp_dict=all_daily_sp_dict, od_cost_dict=od_cost_dict,\n",
    "                 od_cost_inversed_dict=od_cost_inversed_dict, total_cost_base=total_cost_base, \n",
    "                 total_cost_sp_inversed=total_cost_sp_inversed, efficiency_base=efficiency_base, ksp=ksp,\n",
    "                 a_sum_base=a_sum_base, a_master_dict=a_master_dict, flow_df=flow_fromto_df,\n",
    "                 total_cost_base_w=total_cost_base_w, od_cost_dict_w=od_cost_dict_w, sp_cost_dict=sp_cost_dict):\n",
    "    \n",
    "    ff=0\n",
    "    m1_01_dict = {}\n",
    "    m1_02_dict = {}\n",
    "    m2_01_dict = {}\n",
    "    m2_02_dict = {}\n",
    "    m4_02_dict = {}\n",
    "    m6_01_dict = {}\n",
    "    m7_01_dict = {}\n",
    "    m7_02_dict = {}\n",
    "    m7_03_dict = {}\n",
    "    m9_01_dict = {}\n",
    "    \n",
    "    #record all paths in all shortest paths, also record initial total distinct shortest paths\n",
    "    init_n_paths = 0\n",
    "    for key, val in ksp.iteritems():\n",
    "        init_n_paths += val[1]    \n",
    "    \n",
    "    edgelist = []\n",
    "    for edge in list(G.edges()):\n",
    "        edgelist.append(edge)\n",
    "        \n",
    "    for edge in edgelist:\n",
    "        \n",
    "        ff += 1\n",
    "#         if ff%100 == 0:\n",
    "#             print(str(ff) + ' edges have been interdicted')\n",
    "            \n",
    "        u = edge[0]\n",
    "        v = edge[1]\n",
    "        tup = tuple([u,v])\n",
    "        \n",
    "        #make a copy of the original graph\n",
    "        G1 = G.copy()\n",
    "        \n",
    "        #remove that edge\n",
    "        G1.remove_edge(u,v)\n",
    "        \n",
    "        #COPYING DICTIONARY\n",
    "        #for m1\n",
    "        a_dict_base2 = a_dict_base.copy()\n",
    "        a_n_dict_base2 = a_n_dict_base.copy()\n",
    "        #for m2\n",
    "        od_cost_dict2 = od_cost_dict.copy()\n",
    "        od_cost_inversed_dict2 = od_cost_inversed_dict.copy()\n",
    "        #for m4_02\n",
    "        new_n_paths = init_n_paths\n",
    "        #for m6_01\n",
    "        a_master_dict2 = a_master_dict.copy()\n",
    "        #for m7_01\n",
    "        od_cost_dict3 = od_cost_dict_w.copy()\n",
    "        #for m7_02 and m7_03\n",
    "        user_exposure_dict = {}\n",
    "        for centroid in centroids:\n",
    "            user_exposure_dict.update({centroid:[]})\n",
    "        sum_exposure = 0\n",
    "        #for m9_01\n",
    "        unsatisfied_demand = 0\n",
    "        \n",
    "        #M1\n",
    "        for key, val in all_daily_sp_dict.iteritems():\n",
    "            if tup in val:\n",
    "                a_new, a_n_new = _daily_accessibility(centroid=key, G=G1, theta=theta, weight=weight, beta=beta)\n",
    "                a_dict_base2.update({key:a_new})\n",
    "                a_n_dict_base2.update({key:a_n_new})\n",
    "        \n",
    "        #M1 VALUE UPDATE\n",
    "        sum_a_new, sum_a_n_new = _sum_daily_accessibility(a_dict_base2, a_n_dict_base2)\n",
    "        try:\n",
    "            m1_01 = sum_a_base / sum_a_new\n",
    "            m1_01_dict.update({edge:m1_01})\n",
    "            m1_02 = sum_a_n_base / sum_a_n_new\n",
    "            m1_02_dict.update({edge:m1_02})\n",
    "        except:\n",
    "            m1_01 = 9999\n",
    "            m1_01_dict.update({edge:m1_01})\n",
    "            m1_02 = 9999\n",
    "            m1_02_dict.update({edge:m1_02})\n",
    "            print(sum_a_new, sum_a_n_new, sum_a_base, sum_a_n_base)\n",
    "                \n",
    "                \n",
    "        #M2, M6_01, M7_01, M7_02, M7_03, M9_01 ITERATION\n",
    "        updated_centroid = [] #for m6_01\n",
    "        for key, val in sp_dict_graph.iteritems():\n",
    "            if tup in val:\n",
    "                #for m2, m7_01, m9_01\n",
    "                try:\n",
    "                    sp_dijk_distance = nx.dijkstra_path_length(G1, source=key[0], target=key[1], weight=weight)\n",
    "                    cost = sp_dijk_distance\n",
    "                    od_cost_dict2[key] = cost\n",
    "                    od_cost_inversed_dict2[key] = 1 / cost\n",
    "                    flow = od[key[0]][key[1]]\n",
    "                    cost2 = sp_dijk_distance * flow\n",
    "                    od_cost_dict3[key] = cost2\n",
    "                except:\n",
    "                    sp_dijk_distance = 9999\n",
    "                    flow = od[key[0]][key[1]]\n",
    "                    unsatisfied_demand += flow\n",
    "                #for m6_01\n",
    "                try:\n",
    "                    a_source_old_dict = a_master_dict2[key[0]]\n",
    "                    a_source_new_dict = a_source_old_dict.copy()\n",
    "                    a_source_target = betw_w._weighted_accessibility(G=G1, centroid=key[0], targets=key[1], \n",
    "                                                              flow=flow_df, weight=weight, beta=beta)\n",
    "                    a_source_new_dict.update({key[1]:a_source_target})\n",
    "                    a_master_dict2.update({key[0]:a_source_new_dict})\n",
    "                except:\n",
    "                    sp_dijk_distance = 9999\n",
    "                #for m7_02 and m7_03\n",
    "                exposure = betw_w._user_exposure(G_new=G1, centroid=key[0], target=key[1], \n",
    "                                          weight=weight, od=od, sp_cost_dict=sp_cost_dict)\n",
    "                user_exposure_dict[key[0]].append(exposure)\n",
    "                    \n",
    "        #M2 VALUE UPDATE\n",
    "        total_cost_new = sum(od_cost_dict2.values())\n",
    "        total_cost_inversed_new = sum(od_cost_inversed_dict2.values())\n",
    "        efficiency_new = betw_w._network_efficiency_calc(G1, total_cost_inversed_new)\n",
    "        m2_01 = total_cost_new/total_cost_base\n",
    "        if m2_01 < 0:\n",
    "            m2_01 = 0\n",
    "        m2_01_dict.update({edge:m2_01})\n",
    "        try:\n",
    "            m2_02 = (efficiency_base - efficiency_new)/efficiency_base\n",
    "        except:\n",
    "            m2_02 = 9999\n",
    "        m2_02_dict.update({edge:m2_02})\n",
    "        \n",
    "        #M6_01 VALUE UPDATE\n",
    "        a_sum_new = sum([sum(x.values()) for x in a_master_dict2.values()])\n",
    "        try:\n",
    "            m6_01 = a_sum_base/a_sum_new\n",
    "        except:\n",
    "            m6_01 = 9999\n",
    "        if m6_01 < 0:\n",
    "            m6_01 = 0\n",
    "        m6_01_dict.update({edge:m6_01})\n",
    "        \n",
    "        #M7_01 AND M9_01 VALUE UPDATE\n",
    "        total_cost_new = sum(od_cost_dict3.values())\n",
    "        cost_increase = (total_cost_new - total_cost_base_w)/total_cost_base_w\n",
    "        unsatisfied_demand = unsatisfied_demand/total_cost_base_w\n",
    "        if cost_increase < 0:\n",
    "            cost_increase = 0\n",
    "        m7_01_dict.update({edge:cost_increase})\n",
    "        m9_01_dict.update({edge:unsatisfied_demand})\n",
    "        \n",
    "        #M7_02 AND M7_03 VALUE UPDATE\n",
    "        expected_ue_dict = {}\n",
    "        worst_ue_dict = {}\n",
    "        for key, val in user_exposure_dict.iteritems():\n",
    "            if len(val) > 0:\n",
    "                average_val = average(val)\n",
    "                worst_val = max(val)\n",
    "            else:\n",
    "                average_val = 0\n",
    "                worst_val = 0\n",
    "            expected_ue_dict.update({key:average_val})\n",
    "            worst_ue_dict.update({key:worst_val})\n",
    "        try:\n",
    "            m7_02 = sum(expected_ue_dict.values())/len(centroids)\n",
    "            m7_03 = sum(worst_ue_dict.values())/len(centroids)\n",
    "        except:\n",
    "            m7_02 = 9999\n",
    "            m7_03 = 9999\n",
    "        m7_02_dict.update({edge:m7_02})\n",
    "        m7_03_dict.update({edge:m7_03})\n",
    "        \n",
    "        #M4_02 ITERATION\n",
    "        for key, val in ksp.iteritems():\n",
    "            if tup in val[0]:\n",
    "                new_n_paths -= 1\n",
    "                \n",
    "        #M4_02 VALUE UPDATE\n",
    "        try:\n",
    "            m4_02 = 1 - (new_n_paths / init_n_paths)\n",
    "        except:\n",
    "            m4_02 = 9999\n",
    "        m4_02_dict.update({edge:m4_02})        \n",
    "                \n",
    "    #if an edge does not have value yet\n",
    "    #assign 0 to it\n",
    "    all_dicts = [m1_01_dict, m1_02_dict, m2_01_dict, m2_02_dict, m4_02_dict, m6_01_dict, m7_01_dict, m7_02_dict, m7_03_dict, m9_01_dict]\n",
    "    for edge in edgelist:\n",
    "        for metric in all_dicts:\n",
    "            if not edge in metric:\n",
    "                metric.update({edge:0})\n",
    "            \n",
    "    return m1_01_dict, m1_02_dict, m2_01_dict, m2_02_dict, m4_02_dict, m6_01_dict, m7_01_dict, m7_02_dict, m7_03_dict, m9_01_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# m1_01_dict, m1_02_dict, m2_01_dict, m2_02_dict, m4_02_dict, m6_01_dict, m7_01_dict, m7_02_dict, m7_03_dict, m9_01_dict = interdiction(G=G2_new_tograph, centroids=centroid_nodes, od=OD_all, weight='length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%px --local\n",
    "edgelist = []\n",
    "edge_dict = {}\n",
    "for edge in G2_new_tograph.edges():\n",
    "    key = str(edge[0]) + str(edge[1])\n",
    "    edgelist.append(edge)\n",
    "    edge_dict.update({key:[]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMA preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %%px --local\n",
    "theta = 50 #for m1\n",
    "beta = 0.5 #for m1, m6\n",
    "weight='length'\n",
    "cutoff = 0.05 #for m5_01\n",
    "m10_buffer = 0.005 #for m10\n",
    "penalty = 1.2 #for m8_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%px --local\n",
    "def ema_criticality(det_idx=1, od_exp=1, od_loc1=1, od_loc2=1, od_loc3=1, od_loc4=1, theta=50, beta=0.5, weight='length',\n",
    "                   cutoff=0.05, m10_buffer=0.005, penalty=1.2):\n",
    "#     print('here1')\n",
    "    #replicate the graph and gdf to avoid working on the real file\n",
    "    G3 = G2_new_tograph.copy()\n",
    "    gdf3 = gdf2.copy()\n",
    "    \n",
    "#     print('here2')\n",
    "    #set the edge attribute\n",
    "    edge_length_change = {}\n",
    "    for u,v,data in G3.edges(data=True):\n",
    "        length = data['length'] * np.random.uniform(0.75, 1.5)\n",
    "        edge_length_change.update({tuple([u,v]):length})\n",
    "        \n",
    "    nx.set_edge_attributes(G3, 'length', edge_length_change)\n",
    "    \n",
    "#     print('here3')\n",
    "    ##### CREATE OD MATRIX FIRST #####\n",
    "    centroid_nodes = od_p.prepare_centroids_list(G2_new_tograph)\n",
    "\n",
    "    #export OD\n",
    "    prod_lists1 = ['SteelBricks_exp_ton', 'Food_exp_ton','Jutextile_exp_ton', 'Garment_exp_ton']\n",
    "    attr_driver='Total_export'\n",
    "    OD_export_dict = od_p.all_ods_creation_ema(gdf_points = gdf_points, prod_lists = prod_lists1, \n",
    "                                           attr_driver = attr_driver, dist_deterrence = det_func[det_idx])\n",
    "    for key, val in OD_export_dict.iteritems():\n",
    "        OD_export_dict[key] = (OD_export_dict[key][0] * od_exp, OD_export_dict[key][1])\n",
    "    \n",
    "#     print('here4')\n",
    "    prod_lists2 = [ 'Foods_loc_ton', 'Nonfoods_loc_ton']\n",
    "    attr_driver='Population_x'\n",
    "    OD_local_dict1 = od_p.all_ods_creation_ema(gdf_points = gdf_points, prod_lists = prod_lists2, \n",
    "                                           attr_driver = attr_driver, dist_deterrence = det_func[det_idx])\n",
    "    for key, val in OD_local_dict1.iteritems():\n",
    "        OD_local_dict1[key] = (OD_local_dict1[key][0] * od_loc1, OD_local_dict1[key][1])\n",
    "\n",
    "    prod_lists3 = ['RawJute_loc_ton']\n",
    "    attr_driver='Jute_mill'\n",
    "    OD_local_dict2 = od_p.all_ods_creation_ema(gdf_points = gdf_points, prod_lists = prod_lists3, \n",
    "                                           attr_driver = attr_driver, dist_deterrence = det_func[det_idx])\n",
    "    for key, val in OD_local_dict2.iteritems():\n",
    "        OD_local_dict2[key] = (OD_local_dict2[key][0] * od_loc2, OD_local_dict2[key][1])\n",
    "\n",
    "    prod_lists4 = ['Wheat_loc_ton']\n",
    "    attr_driver='Flour_mill'\n",
    "    OD_local_dict3 = od_p.all_ods_creation_ema(gdf_points = gdf_points, prod_lists = prod_lists4, \n",
    "                                           attr_driver = attr_driver, dist_deterrence = det_func[det_idx])\n",
    "    for key, val in OD_local_dict3.iteritems():\n",
    "        OD_local_dict3[key] = (OD_local_dict3[key][0] * od_loc3, OD_local_dict3[key][1])\n",
    "\n",
    "    prod_lists5 = ['Textile_loc_ton']\n",
    "    attr_driver='Tot_Garment_Factory'\n",
    "    OD_local_dict4 = od_p.all_ods_creation_ema(gdf_points = gdf_points, prod_lists = prod_lists5, \n",
    "                                           attr_driver = attr_driver, dist_deterrence = det_func[det_idx])\n",
    "    for key, val in OD_local_dict4.iteritems():\n",
    "        OD_local_dict4[key] = (OD_local_dict4[key][0] * od_loc4, OD_local_dict4[key][1])\n",
    "\n",
    "#     print('here5')\n",
    "    OD_all_dict = od_p._merge_two_dicts(OD_export_dict, OD_local_dict1)\n",
    "    OD_all_dict = od_p._merge_two_dicts(OD_all_dict, OD_local_dict2)\n",
    "    OD_all_dict = od_p._merge_two_dicts(OD_all_dict, OD_local_dict3)\n",
    "    OD_all_dict = od_p._merge_two_dicts(OD_all_dict, OD_local_dict4)\n",
    "\n",
    "    prod_lists = prod_lists1+prod_lists2+prod_lists3+prod_lists4+prod_lists5\n",
    "\n",
    "    factors_dict = od_p.factors_dict_creation(prod_lists)\n",
    "\n",
    "    #combine all od\n",
    "    OD_all = od_aggregation(OD_all_dict, **factors_dict)\n",
    "    \n",
    "    print(OD_all.sum().sum())\n",
    "    \n",
    "#     print('here6')\n",
    "    ##### THEN PREPARE ALL THE SUPPORTING THINGS #####\n",
    "    \n",
    "    #EMA building block\n",
    "    sp_dict_graph = betw_w.sp_dict_graph_creation(G=G3, sources=centroid_nodes, \n",
    "                                                  targets=centroid_nodes, weight='length')\n",
    "    a_dict_base, a_n_dict_base = _dict_daily_accessibility(centroids=centroid_nodes, G=G3, theta=theta, \n",
    "                                                                     weight='length', beta=beta)\n",
    "    sum_a_base, sum_a_n_base = _sum_daily_accessibility(a_dict_base, a_n_dict_base)\n",
    "    all_daily_sp_list, all_daily_sp_dict = betw_w._all_daily_sp_record(G=G3, sources=centroid_nodes, \n",
    "                                                                cutoff=theta, weight='length')\n",
    "\n",
    "#     print('here7')\n",
    "    #EMA building block\n",
    "    div_init_avrgdist_dict = {}\n",
    "    for key, val in div_graph_dict.iteritems():\n",
    "        cc = 0\n",
    "        if nx.number_connected_components(val) > 1:\n",
    "            for subgraph in nx.connected_component_subgraphs(val):\n",
    "                if len(subgraph) > cc:\n",
    "                    cc = len(subgraph)\n",
    "                    graph = subgraph\n",
    "        else:\n",
    "            graph = val\n",
    "        average_sp_length = nx.average_shortest_path_length(graph, weight='length')\n",
    "        div_init_avrgdist_dict.update({key:average_sp_length})\n",
    "\n",
    "#     print('here8')\n",
    "    #EMA building block\n",
    "    total_cost_base, od_cost_dict = betw_w._total_cost_sp(G=G3, sources=centroid_nodes, targets=centroid_nodes,\n",
    "                                                  weight=weight, od=OD_all, weighted=False)\n",
    "    total_cost_base_w, od_cost_dict_w = betw_w._total_cost_sp(G=G3, sources=centroid_nodes, targets=centroid_nodes,\n",
    "                                                  weight=weight, od=OD_all, weighted=True)\n",
    "    total_cost_sp_inversed, od_cost_inversed_dict =  betw_w._total_cost_sp_inversed(G=G3, sources=centroid_nodes, \n",
    "                                                                             targets=centroid_nodes, weight=weight)\n",
    "    efficiency_base = betw_w._network_efficiency_calc(G=G3, total_cost_inversed=total_cost_sp_inversed)\n",
    "\n",
    "#     print('here9')\n",
    "    #EMA building block\n",
    "    flow_fromto_df = pd.DataFrame(OD_all.sum(axis=0)+OD_all.sum(axis=1))\n",
    "    flow_fromto_df.columns = ['flow']\n",
    "    a_sum_base, a_master_dict =  betw_w._sum_weighted_accessibility(G=G3, centroids=centroid_nodes,\n",
    "                                                                    flow=flow_fromto_df, weight='length', beta=beta)\n",
    "\n",
    "    #EMA building block\n",
    "    sp_cost_dict = betw_w._shortest_path_cost(G=G3, centroids=centroid_nodes, weight='length')\n",
    "\n",
    "#     print('here10')\n",
    "    #EMA building block\n",
    "    flood_file = 'fluvial_defended_1in75_tile_1_-9999to0.tif'\n",
    "    gdf4 = gdf3.copy()\n",
    "    gdf4['geometry'] = gdf4.geometry.apply(lambda geom: geom.buffer(m10_buffer))\n",
    "    zonal_stats_dict = zonal_stats(gdf4, flood_file, stats='mean', all_touched=False)\n",
    "    \n",
    "    print('here11')\n",
    "    #### THEN CALCULATE THE METRICS ####\n",
    "    m1_01_dict, m1_02_dict, m2_01_dict, m2_02_dict, m4_02_dict, m6_01_dict, m7_01_dict, m7_02_dict, m7_03_dict, m9_01_dict = interdiction(G=G3, centroids=centroid_nodes, od=OD_all, \n",
    "                                                                                                                                          weight='length', theta=theta, beta=beta,\n",
    "                                                                                                                                          a_dict_base=a_dict_base, a_n_dict_base=a_n_dict_base, \n",
    "                                                                                                                                          sum_a_base=sum_a_base, sum_a_n_base=sum_a_n_base,\n",
    "                                                                                                                                          all_daily_sp_list=all_daily_sp_list, \n",
    "                                                                                                                                          all_daily_sp_dict=all_daily_sp_dict, \n",
    "                                                                                                                                          od_cost_dict=od_cost_dict, \n",
    "                                                                                                                                          od_cost_inversed_dict=od_cost_inversed_dict, \n",
    "                                                                                                                                          total_cost_base=total_cost_base, \n",
    "                                                                                                                                          total_cost_sp_inversed=total_cost_sp_inversed, \n",
    "                                                                                                                                          efficiency_base=efficiency_base, ksp=ksp,\n",
    "                                                                                                                                          a_sum_base=a_sum_base, a_master_dict=a_master_dict, \n",
    "                                                                                                                                          flow_df=flow_fromto_df, total_cost_base_w=total_cost_base_w, \n",
    "                                                                                                                                          od_cost_dict_w=od_cost_dict_w, sp_cost_dict=sp_cost_dict)\n",
    "    \n",
    "    m3_01_dict, m3_02_dict, m5_01_dict, m8_02_dict, m10_dict = no_interdiction(G=G3, centroids=centroid_nodes, \n",
    "                                                                               od=OD_all, od_unw=OD_unweighted, weight='length', \n",
    "                                                                               gdf=gdf3, div_graph_dict=div_graph_dict, div_init_avrgdist_dict=div_init_avrgdist_dict, \n",
    "                                                                               cutoff=cutoff, penalty=penalty, zonal_stats_dict=zonal_stats_dict)\n",
    "    print('here12')\n",
    "\n",
    "    \n",
    "    ##### COMBINE ALL DICTIONARIES ####\n",
    "    all_dicts = [m3_01_dict, m3_02_dict, m5_01_dict, m8_02_dict, m10_dict, m1_01_dict, m1_02_dict, m2_01_dict, m2_02_dict, \n",
    "                 m4_02_dict, m6_01_dict, m7_01_dict, m7_02_dict, m7_03_dict, m9_01_dict]\n",
    "    d = {}\n",
    "    for k in m3_01_dict.iterkeys():\n",
    "        d[k] = tuple(d[k] for d in all_dicts)\n",
    "        \n",
    "    edge_dict = {}\n",
    "    for key,val in d.iteritems():\n",
    "        keyn = str(key[0]) + str(key[1])\n",
    "        edge_dict.update({keyn:val})\n",
    "        \n",
    "    return edge_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def od_aggregation(OD_all_dict, **factors_dict):\n",
    "    #create empty dictionary\n",
    "    OD_final_dict={}\n",
    "\n",
    "    #iterate over all items in original OD\n",
    "    for key1,val1 in OD_all_dict.iteritems():\n",
    "\n",
    "        #matching the production value of the OD dict and the factors_dict\n",
    "        for key2,val2 in factors_dict.iteritems():\n",
    "            #if it is a match\n",
    "            if val1[1] == key2:\n",
    "                #scale the OD flows of that particular product\n",
    "                OD_final_dict[\"od_{0}\".format(val1[1])]=val1[0]*val2\n",
    "\n",
    "    #creation of final OD dataframe\n",
    "    OD_final_df = OD_final_dict[OD_final_dict.keys()[0]]\n",
    "    OD_final_df.fillna(value=0, inplace=True)\n",
    "    for i in range(len(OD_final_dict)-1):\n",
    "        OD_new = OD_final_dict[OD_final_dict.keys()[i+1]]\n",
    "        OD_new.fillna(value=0, inplace=True)\n",
    "        OD_final_df = OD_final_df +  OD_new\n",
    "\n",
    "    return OD_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import datetime\n",
    "# timestart = datetime.datetime.now()\n",
    "# print(timestart)\n",
    "\n",
    "# dictall = ema_criticality()\n",
    "\n",
    "# timeend = datetime.datetime.now()\n",
    "# print(timeend-timestart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:/Users/User/Desktop/BramkaFile/bangladesh_network\\od_prep.py:137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  relative_attr[i[0]] = i[1] * np.random.uniform(0.5, 1.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here6\n",
      "here11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here12\n"
     ]
    }
   ],
   "source": [
    "# %%px --local\n",
    "edge_dict = ema_criticality()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## EMA run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%px --local\n",
    "from ema_workbench import (Model, RealParameter, ScalarOutcome, Constant, IntegerParameter)\n",
    "\n",
    "#instantiate the model\n",
    "criticality_model = Model('criticality', function = ema_criticality)\n",
    "\n",
    "#specify uncertainties\n",
    "criticality_model.uncertainties = [IntegerParameter('det_idx', 1, 3),\n",
    "                            RealParameter('od_exp', 0.75, 1.5),\n",
    "                            RealParameter('od_loc1', 0.75, 1.5),\n",
    "                            RealParameter('od_loc2', 0.75, 1.5),\n",
    "                            RealParameter('od_loc3', 0.75, 1.5),\n",
    "                            RealParameter('od_loc4', 0.75, 1.5),\n",
    "                            RealParameter('theta', 30, 70),\n",
    "                            RealParameter('beta', 0.25, 0.75),\n",
    "                            RealParameter('cutoff', 0.025, 0.075),\n",
    "                            RealParameter('m10_buffer', 0.0025, 0.0075),\n",
    "                            RealParameter('penalty', 1, 1.5)]\n",
    "\n",
    "#specify outcomes \n",
    "criticality_model.outcomes = [ScalarOutcome(key) for key,val in edge_dict.iteritems()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1258"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edge_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edge_dict = load_obj('edge_dict_ema')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crit_df = pd.DataFrame.from_dict(edge_dict, orient='index')\n",
    "crit_df2 = crit_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crit_df['osmid'] = crit_df.index\n",
    "crit_df.index = np.arange(0, len(crit_df), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = 0\n",
    "crit_df['rep'] = [c for i in np.arange(0,len(crit_df),1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# c = 0\n",
    "# while c < 3:\n",
    "    \n",
    "#     new_df = crit_df2.copy()\n",
    "#     new_df['osmid'] = new_df.index\n",
    "#     new_df.index = np.arange(0, len(new_df), 1)\n",
    "#     new_df['rep'] = [c+1 for i in np.arange(0,len(new_df),1)]\n",
    "#     crit_df = crit_df.append(new_df)\n",
    "#     c += 1\n",
    "#     crit_df.to_csv('version_{}.csv'.format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "timestart = datetime.datetime.now()\n",
    "print(timestart)\n",
    "\n",
    "m3_01_dict, m3_02_dict, m5_01_dict, m8_02_dict, m10_dict = no_interdiction(G=G2_new_tograph, centroids=centroid_nodes, \n",
    "                                                                           od=OD_all, od_unw=OD_unweighted, weight='length')\n",
    "\n",
    "timeend = datetime.datetime.now()\n",
    "print(timeend-timestart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:/Users/Lenovo/Desktop/bangladesh_network\\od_prep.py:137: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  relative_attr[i[0]] = i[1] * np.random.uniform(0.5, 1.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80983494.698\n",
      "here11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here12\n",
      "2:15:29.593000\n",
      "82441076.9093\n",
      "here11\n",
      "here12\n",
      "2:26:50.926000\n",
      "83695773.9975\n",
      "here11\n",
      "here12\n",
      "2:22:12.914000\n",
      "54311036.8575\n",
      "here11\n",
      "here12\n",
      "2:26:37.918000\n",
      "87600361.6121\n",
      "here11\n",
      "here12\n",
      "2:36:00.668000\n",
      "60710082.8756\n",
      "here11\n",
      "here12\n",
      "2:24:11.503000\n",
      "81427135.8291\n",
      "here11\n",
      "here12\n",
      "2:15:54.124000\n",
      "74892445.3533\n",
      "here11\n"
     ]
    }
   ],
   "source": [
    "#non-EMA run\n",
    "c = 0\n",
    "import datetime\n",
    "while c < 100:\n",
    "    timestart = datetime.datetime.now()\n",
    "    det_idx = np.random.randint(1,4)\n",
    "    od_exp = np.random.uniform(0.75,1.5)\n",
    "    od_loc1 = np.random.uniform(0.75,1.5)\n",
    "    od_loc2 = np.random.uniform(0.75,1.5)\n",
    "    od_loc3 = np.random.uniform(0.75,1.5)\n",
    "    od_loc4 = np.random.uniform(0.75,1.5)\n",
    "    theta = np.random.randint(30,71)\n",
    "    beta = np.random.uniform(0.25, 0.75)\n",
    "    cutoff = np.random.uniform(0.025, 0.075)\n",
    "    m10_buffer = np.random.uniform(0.0025, 0.0075)\n",
    "    penalty = np.random.uniform(1, 1.5)\n",
    "    \n",
    "    new_dict = ema_criticality(det_idx=det_idx, od_exp=od_exp, od_loc1=od_loc1, od_loc2=od_loc2, \n",
    "                               od_loc3=od_loc3, od_loc4=od_loc4, theta=theta, beta=beta, weight='length',\n",
    "                               cutoff=cutoff, m10_buffer=m10_buffer, penalty=penalty)\n",
    "    \n",
    "    new_df = pd.DataFrame.from_dict(new_dict, orient='index')\n",
    "    new_df['osmid'] = new_df.index\n",
    "    new_df.index = np.arange(0, len(new_df), 1)\n",
    "    repnum = c + 17\n",
    "    new_df['rep'] = [repnum+1 for i in np.arange(0,len(new_df),1)]\n",
    "    crit_df = crit_df.append(new_df)\n",
    "    c += 1\n",
    "    repnum = c + 17\n",
    "    new_df.to_csv('result_single_version_{}.csv'.format(repnum))\n",
    "    crit_df.to_csv('result_version_{}.csv'.format(repnum))\n",
    "    timeend = datetime.datetime.now()\n",
    "    print(timeend-timestart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-22 16:43:05.682000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] performing experiments using ipyparallel\n",
      "[MainProcess/INFO] performing 100 scenarios * 1 policies * 1 model(s) = 100 experiments\n"
     ]
    }
   ],
   "source": [
    "from ema_workbench import Policy, perform_experiments\n",
    "from ema_workbench import ema_logging, save_results\n",
    "from ema_workbench.em_framework import IpyparallelEvaluator\n",
    "\n",
    "import datetime\n",
    "timestart = datetime.datetime.now()\n",
    "print(timestart)\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "n_scenarios = 100\n",
    "with IpyparallelEvaluator(criticality_model, client=client) as evaluator:\n",
    "    results = evaluator.perform_experiments(scenarios=n_scenarios)\n",
    "\n",
    "fh =  r'./data/{} experiments_22may_v01.tar.gz'.format(n_scenarios)\n",
    "save_results(results, fh)\n",
    "\n",
    "timeend = datetime.datetime.now()\n",
    "print(timeend-timestart)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] performing 10 scenarios * 1 policies * 1 model(s) = 10 experiments\n",
      "[MainProcess/INFO] performing experiments sequentially\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here1\n",
      "here6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:19: RuntimeWarning: divide by zero encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:130: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\User\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:191: RuntimeWarning: invalid value encountered in double_scalars\n",
      "C:\\Users\\User\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\Anaconda2\\lib\\site-packages\\ema_workbench\\em_framework\\experiment_runner.py\", line 89, in run_experiment\n",
      "    model.run_model(scenario, policy)\n",
      "  File \"C:\\Users\\User\\Anaconda2\\lib\\site-packages\\ema_workbench\\util\\ema_logging.py\", line 49, in wrapper\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\Anaconda2\\lib\\site-packages\\ema_workbench\\em_framework\\model.py\", line 356, in run_model\n",
      "    self.output = self.run_experiment(experiment)\n",
      "  File \"C:\\Users\\User\\Anaconda2\\lib\\site-packages\\ema_workbench\\util\\ema_logging.py\", line 49, in wrapper\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"C:\\Users\\User\\Anaconda2\\lib\\site-packages\\ema_workbench\\em_framework\\model.py\", line 410, in run_experiment\n",
      "    model_output = self.function(**experiment)\n",
      "  File \"<ipython-input-14-c68384b6e669>\", line 146, in ema_criticality\n",
      "    cutoff=cutoff, penalty=penalty, zonal_stats_dict=zonal_stats_dict)\n",
      "  File \"<ipython-input-20-f9f16f8d82e2>\", line 36, in no_interdiction\n",
      "    m8_02_dict = betw_w.edge_betweenness_centrality(flow_probit_5, od)\n",
      "  File \"C:/Users/User/Desktop/BramkaFile/bangladesh_network\\weighted_betweenness.py\", line 269, in edge_betweenness_centrality\n",
      "    flow2[key] = val / totalval\n",
      "ZeroDivisionError: float division by zero\n"
     ]
    },
    {
     "ename": "EMAError",
     "evalue": "exception in run_model\nCaused by: ZeroDivisionError: float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEMAError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-036d2a7013a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mn_scenarios\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mperform_experiments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcriticality_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_scenarios\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;34mr'./data/{} experiments_22may_v01.tar.gz'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_scenarios\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\User\\Anaconda2\\lib\\site-packages\\ema_workbench\\em_framework\\evaluators.pyc\u001b[0m in \u001b[0;36mperform_experiments\u001b[0;34m(models, scenarios, policies, evaluator, reporting_interval, uncertainty_union, lever_union, outcome_union, uncertainty_sampling, levers_sampling)\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mevaluator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequentialEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m     \u001b[0mevaluator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_experiments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscenarios\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mnr_of_exp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\User\\Anaconda2\\lib\\site-packages\\ema_workbench\\em_framework\\evaluators.pyc\u001b[0m in \u001b[0;36mevaluate_experiments\u001b[0;34m(self, scenarios, policies, callback)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mrunner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExperimentRunner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mex_gen\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_experiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcleanup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\User\\Anaconda2\\lib\\site-packages\\ema_workbench\\em_framework\\experiment_runner.pyc\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(self, experiment)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             raise EMAError((\"exception in run_model\"\n\u001b[0;32m--> 104\u001b[0;31m                    \"\\nCaused by: {}: {}\".format(type(e).__name__, str(e))))\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEMAError\u001b[0m: exception in run_model\nCaused by: ZeroDivisionError: float division by zero"
     ]
    }
   ],
   "source": [
    "from ema_workbench import Policy, perform_experiments\n",
    "from ema_workbench import ema_logging, save_results\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "n_scenarios = 10\n",
    "results = perform_experiments(criticality_model, n_scenarios)\n",
    "\n",
    "fh =  r'./data/{} experiments_22may_v01.tar.gz'.format(n_scenarios)\n",
    "save_results(results, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ema_workbench import Policy, perform_experiments\n",
    "from ema_workbench import ema_logging, save_results\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "n_scenarios = 20\n",
    "results = perform_experiments(criticality_model, n_scenarios)\n",
    "\n",
    "fh =  r'./data/{} experiments_22may_v02.tar.gz'.format(n_scenarios)\n",
    "save_results(results, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ema_workbench import Policy, perform_experiments\n",
    "from ema_workbench import ema_logging, save_results\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "n_scenarios = 40\n",
    "results = perform_experiments(criticality_model, n_scenarios)\n",
    "\n",
    "fh =  r'./data/{} experiments_22may_v03.tar.gz'.format(n_scenarios)\n",
    "save_results(results, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from ema_workbench import Policy, perform_experiments\n",
    "from ema_workbench import ema_logging, save_results\n",
    "\n",
    "ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "\n",
    "n_scenarios = 30\n",
    "results = perform_experiments(criticality_model, n_scenarios)\n",
    "\n",
    "fh =  r'./data/{} experiments_22may_v04.tar.gz'.format(n_scenarios)\n",
    "save_results(results, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[MainProcess/INFO] results saved successfully to C:\\Users\\User\\Desktop\\BramkaFile\\betweenness_ema_thesis\\data\\3 experiments_22may_v01.tar.gz\n"
     ]
    }
   ],
   "source": [
    "from ema_workbench import save_results\n",
    "fh =  r'./data/{} experiments_22may_v01.tar.gz'.format(n_scenarios)\n",
    "save_results(results, fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## used for EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%px --local\n",
    "#EMA BUILDING BLOCK\n",
    "\n",
    "\n",
    "centroid_nodes = od_p.prepare_centroids_list(G2_new_tograph)\n",
    "\n",
    "#export OD\n",
    "prod_lists1 = ['SteelBricks_exp_ton', 'Food_exp_ton','Jutextile_exp_ton', 'Garment_exp_ton']\n",
    "attr_driver='Total_export'\n",
    "OD_export_dict = od_p.all_ods_creation_ema(gdf_points = gdf_points, prod_lists = prod_lists1, \n",
    "                                       attr_driver = attr_driver, dist_deterrence = det_func)\n",
    "\n",
    "prod_lists2 = [ 'Foods_loc_ton', 'Nonfoods_loc_ton']\n",
    "attr_driver='Population_x'\n",
    "OD_local_dict1 = od_p.all_ods_creation_ema(gdf_points = gdf_points, prod_lists = prod_lists2, \n",
    "                                       attr_driver = attr_driver, dist_deterrence = det_func)\n",
    "\n",
    "prod_lists3 = ['RawJute_loc_ton']\n",
    "attr_driver='Jute_mill'\n",
    "OD_local_dict2 = od_p.all_ods_creation_ema(gdf_points = gdf_points, prod_lists = prod_lists3, \n",
    "                                       attr_driver = attr_driver, dist_deterrence = det_func)\n",
    "\n",
    "prod_lists4 = ['Wheat_loc_ton']\n",
    "attr_driver='Flour_mill'\n",
    "OD_local_dict3 = od_p.all_ods_creation_ema(gdf_points = gdf_points, prod_lists = prod_lists4, \n",
    "                                       attr_driver = attr_driver, dist_deterrence = det_func)\n",
    "\n",
    "prod_lists5 = ['Textile_loc_ton']\n",
    "attr_driver='Tot_Garment_Factory'\n",
    "OD_local_dict4 = od_p.all_ods_creation_ema(gdf_points = gdf_points, prod_lists = prod_lists5, \n",
    "                                       attr_driver = attr_driver, dist_deterrence = det_func)\n",
    "\n",
    "OD_all_dict = od_p._merge_two_dicts(OD_export_dict, OD_local_dict1)\n",
    "OD_all_dict = od_p._merge_two_dicts(OD_all_dict, OD_local_dict2)\n",
    "OD_all_dict = od_p._merge_two_dicts(OD_all_dict, OD_local_dict3)\n",
    "OD_all_dict = od_p._merge_two_dicts(OD_all_dict, OD_local_dict4)\n",
    "\n",
    "prod_lists = prod_lists1+prod_lists2+prod_lists3+prod_lists4+prod_lists5\n",
    "\n",
    "factors_dict = od_p.factors_dict_creation(prod_lists)\n",
    "\n",
    "# sp_dict = betw_w._shortest_path_record(G=G2_new_tograph, sources=centroid_nodes, \n",
    "#                                      targets=centroid_nodes, weight='length')\n",
    "   \n",
    "#combine all od\n",
    "OD_all = od_p.od_aggregation(OD_all_dict, **factors_dict)\n",
    "\n",
    "#create unweighted OD\n",
    "centroid_district_listed = list(OD_all.columns)\n",
    "\n",
    "OD_unweighted = pd.DataFrame(1, index=centroid_district_listed, columns=centroid_district_listed)\n",
    "\n",
    "for i,row in OD_unweighted.iterrows():\n",
    "    OD_unweighted.loc[i][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#EMA building block\n",
    "sp_dict_graph = betw_w.sp_dict_graph_creation(G=G2_new_tograph, sources=centroid_nodes, \n",
    "                                              targets=centroid_nodes, weight='length')\n",
    "a_dict_base, a_n_dict_base = betw_w._dict_daily_accessibility(centroids=centroid_nodes, G=G2_new_tograph, theta=theta, \n",
    "                                                                 weight='length', beta=beta)\n",
    "sum_a_base, sum_a_n_base = betw_w._sum_daily_accessibility(a_dict_base, a_n_dict_base)\n",
    "all_daily_sp_list, all_daily_sp_dict = betw_w._all_daily_sp_record(G=G2_new_tograph, sources=centroid_nodes, \n",
    "                                                            cutoff=theta, weight='length')\n",
    "\n",
    "#EMA building block\n",
    "div_graph_list = []\n",
    "for div in division_list:\n",
    "    exec(\"{}_graph = nx.from_pandas_dataframe(df={}_gdf, source='FNODE_', target='TNODE_',edge_attr='length')\".format(div,div))\n",
    "    exec(\"div_graph_list.append({}_graph)\".format(div))\n",
    "div_graph_dict = dict(zip(division_list, div_graph_list))\n",
    "div_init_avrgdist_dict = {}\n",
    "for key, val in div_graph_dict.iteritems():\n",
    "    cc = 0\n",
    "    if nx.number_connected_components(val) > 1:\n",
    "        for subgraph in nx.connected_component_subgraphs(val):\n",
    "            if len(subgraph) > cc:\n",
    "                cc = len(subgraph)\n",
    "                graph = subgraph\n",
    "    else:\n",
    "        graph = val\n",
    "    average_sp_length = nx.average_shortest_path_length(graph, weight='length')\n",
    "    div_init_avrgdist_dict.update({key:average_sp_length})\n",
    "    \n",
    "#EMA building block\n",
    "total_cost_base, od_cost_dict = betw_w._total_cost_sp(G=G2_new_tograph, sources=centroid_nodes, targets=centroid_nodes,\n",
    "                                              weight=weight, od=OD_all, weighted=False)\n",
    "total_cost_base_w, od_cost_dict_w = betw_w._total_cost_sp(G=G2_new_tograph, sources=centroid_nodes, targets=centroid_nodes,\n",
    "                                              weight=weight, od=OD_all, weighted=True)\n",
    "total_cost_sp_inversed, od_cost_inversed_dict =  betw_w._total_cost_sp_inversed(G=G2_new_tograph, sources=centroid_nodes, \n",
    "                                                                         targets=centroid_nodes, weight=weight)\n",
    "efficiency_base = betw_w._network_efficiency_calc(G=G2_new_tograph, total_cost_inversed=total_cost_sp_inversed)\n",
    "\n",
    "#EMA building block\n",
    "flow_fromto_df = pd.DataFrame(OD_all.sum(axis=0)+OD_all.sum(axis=1))\n",
    "flow_fromto_df.columns = ['flow']\n",
    "a_sum_base, a_master_dict =  betw_w._sum_weighted_accessibility(G=G2_new_tograph, centroids=centroid_nodes,\n",
    "                                                                flow=flow_fromto_df, weight='length', beta=beta)\n",
    "\n",
    "#EMA building block\n",
    "sp_cost_dict = betw_w._shortest_path_cost(G=G2_new_tograph, centroids=centroid_nodes, weight='length')\n",
    "\n",
    "#EMA building block\n",
    "flood_file = 'fluvial_defended_1in75_tile_1_-9999to0.tif'\n",
    "gdf3 = gdf2.copy()\n",
    "gdf3['geometry'] = gdf3.geometry.apply(lambda geom: geom.buffer(m10_buffer))\n",
    "zonal_stats_dict = zonal_stats(gdf3, flood_file, stats='mean', all_touched=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## real deal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-21 07:48:38.943000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:24:19.479000\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "timestart = datetime.datetime.now()\n",
    "print(timestart)\n",
    "\n",
    "m3_01_dict, m3_02_dict, m5_01_dict, m8_02_dict, m10_dict = no_interdiction(G=G2_new_tograph, centroids=centroid_nodes, \n",
    "                                                                           od=OD_all, od_unw=OD_unweighted, weight='length')\n",
    "\n",
    "timeend = datetime.datetime.now()\n",
    "print(timeend-timestart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for dict merger\n",
    "all_dicts = [m3_01_dict, m3_02_dict, \n",
    "             m5_01_dict, m8_02_dict, m10_dict]\n",
    "d = {}\n",
    "for k in m3_01_dict.iterkeys():\n",
    "    d[k] = tuple(d[k] for d in all_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258\n",
      "1258\n",
      "1258\n",
      "1258\n",
      "1258\n"
     ]
    }
   ],
   "source": [
    "alll = [m3_01_dict, m3_02_dict, m5_01_dict, m8_02_dict, m10_dict]\n",
    "for i in alll:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for num, i in enumerate(alll):\n",
    "    for key, val in i.iteritems():\n",
    "        for num2, j in enumerate(alll):\n",
    "            if i != j:\n",
    "                if not key in j.keys():\n",
    "                    print('key ' + str(key) + ' in ' + str(i) + ' not available in ' + str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdf_final, m1_01_df = betw_w.betweenness_to_df(gdf2,m3_01_dict,'m3_01')\n",
    "gdf_final, m1_01_df = betw_w.betweenness_to_df(gdf_final,m3_02_dict,'m3_02')\n",
    "gdf_final, m1_01_df = betw_w.betweenness_to_df(gdf_final,m5_01_dict,'m5_01')\n",
    "gdf_final, m1_01_df = betw_w.betweenness_to_df(gdf_final,m8_02_dict,'m8_02')\n",
    "gdf_final, m1_01_df = betw_w.betweenness_to_df(gdf_final,m10_dict,'m10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdf_final.to_csv('result_NoInterdiction_1107noz2_v01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-20 00:40:33.471000\n",
      "starting the interdiction\n",
      "5 edges have been interdicted\n",
      "50 edges have been interdicted\n",
      "100 edges have been interdicted\n",
      "150 edges have been interdicted\n",
      "200 edges have been interdicted\n",
      "250 edges have been interdicted\n",
      "300 edges have been interdicted\n",
      "350 edges have been interdicted\n",
      "400 edges have been interdicted\n",
      "450 edges have been interdicted\n",
      "500 edges have been interdicted\n",
      "550 edges have been interdicted\n",
      "600 edges have been interdicted\n",
      "650 edges have been interdicted\n",
      "700 edges have been interdicted\n",
      "750 edges have been interdicted\n",
      "800 edges have been interdicted\n",
      "850 edges have been interdicted\n",
      "900 edges have been interdicted\n",
      "950 edges have been interdicted\n",
      "1000 edges have been interdicted\n",
      "1050 edges have been interdicted\n",
      "1100 edges have been interdicted\n",
      "1150 edges have been interdicted\n",
      "1200 edges have been interdicted\n",
      "1250 edges have been interdicted\n",
      "1:21:30.666000\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "timestart = datetime.datetime.now()\n",
    "print(timestart)\n",
    "\n",
    "m1_01_dict, m1_02_dict, m2_01_dict, m2_02_dict, m4_02_dict, m6_01_dict, m7_01_dict, m7_02_dict, m7_03_dict, m9_01_dict = interdiction(G=G2_new_tograph, centroids=centroid_nodes, od=OD_all, weight='length')\n",
    "\n",
    "timeend = datetime.datetime.now()\n",
    "print(timeend-timestart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdf_final, m1_01_df = betw_w.betweenness_to_df(gdf2,m1_01_dict,'m1_01')\n",
    "gdf_final, m1_01_df = betw_w.betweenness_to_df(gdf_final,m1_02_dict,'m1_02')\n",
    "gdf_final, m1_01_df = betw_w.betweenness_to_df(gdf_final,m2_01_dict,'m2_01')\n",
    "gdf_final, m1_01_df = betw_w.betweenness_to_df(gdf_final,m2_02_dict,'m2_02')\n",
    "gdf_final, m1_01_df = betw_w.betweenness_to_df(gdf_final,m4_02_dict,'m4_02')\n",
    "gdf_final, m1_01_df = betw_w.betweenness_to_df(gdf_final,m6_01_dict,'m6_01')\n",
    "gdf_final, m1_01_df = betw_w.betweenness_to_df(gdf_final,m7_01_dict,'m7_01')\n",
    "gdf_final, m1_01_df = betw_w.betweenness_to_df(gdf_final,m7_02_dict,'m7_02')\n",
    "gdf_final, m1_01_df = betw_w.betweenness_to_df(gdf_final,m7_03_dict,'m7_03')\n",
    "gdf_final, m1_01_df = betw_w.betweenness_to_df(gdf_final,m9_01_dict,'m9_01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdf_final.to_csv('result_interdiction_1107noz2_v03.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1258\n",
      "1258\n",
      "1258\n",
      "1258\n",
      "1258\n",
      "1258\n",
      "1258\n",
      "1258\n",
      "1258\n",
      "1258\n"
     ]
    }
   ],
   "source": [
    "alll = [m1_01_dict, m1_02_dict, m2_01_dict, m2_02_dict, m4_02_dict, m6_01_dict, m7_01_dict, m7_02_dict, m7_03_dict, m9_01_dict]\n",
    "for i in alll:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "check = [x for x in m1_02_dict.values() if x > 0]\n",
    "keys= [x for x in m1_02_dict.keys() if m1_02_dict[x] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1258"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m7_02_dict)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
